{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b77d87bb-0073-43cd-81ad-23ad494f1033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dspy\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff8c360d-6a87-4bab-83ce-208bdf92fad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "üì∫ To view the Phoenix app in a notebook, run `px.active_session().view()`\n",
      "üìñ For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    }
   ],
   "source": [
    "import phoenix as px\n",
    "phoenix_session = px.launch_app()\n",
    "\n",
    "from openinference.instrumentation.dspy import DSPyInstrumentor\n",
    "from opentelemetry import trace as trace_api\n",
    "from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n",
    "from opentelemetry.sdk import trace as trace_sdk\n",
    "from opentelemetry.sdk.resources import Resource\n",
    "from opentelemetry.sdk.trace.export import SimpleSpanProcessor\n",
    "\n",
    "endpoint = \"http://127.0.0.1:6006/v1/traces\"\n",
    "resource = Resource(attributes={})\n",
    "tracer_provider = trace_sdk.TracerProvider(resource=resource)\n",
    "span_otlp_exporter = OTLPSpanExporter(endpoint=endpoint)\n",
    "tracer_provider.add_span_processor(SimpleSpanProcessor(span_exporter=span_otlp_exporter))\n",
    "\n",
    "trace_api.set_tracer_provider(tracer_provider=tracer_provider)\n",
    "DSPyInstrumentor().instrument()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1571b851-d883-4de0-9915-078cc2f46a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3608, 5), (1120, 5))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"yqzheng/semeval2014_restaurants\")\n",
    "import pandas as pd\n",
    "train = pd.DataFrame(dataset['train'])\n",
    "test = pd.DataFrame(dataset['test'])\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d783dc4-9680-4540-ac22-693c3535b4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_jsons(json_list):\n",
    "    result = {}\n",
    "    for j in json_list:\n",
    "        result.update(j)\n",
    "    return result\n",
    "\n",
    "def create_json(df):\n",
    "    df['json'] = df.apply(lambda row: {row['aspect']: row['label']} , axis=1)\n",
    "    return df.groupby('text')['json'].agg(merge_jsons).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bef126d6-4910-49b6-8c66-cef987fdf511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$160 for 2 filets, 2 sides, an appetizer and d...</td>\n",
       "      <td>{'filets': 0, 'sides': 0, 'appetizer': 0, 'dri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$20 for all you can eat sushi cannot be beaten.</td>\n",
       "      <td>{'sushi': 0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$20 gets you unlimited sushi of a very high qu...</td>\n",
       "      <td>{'sushi': 1, 'sushi places': 1, 'quality': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$6 and there is much tasty food, all of it fre...</td>\n",
       "      <td>{'food': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>($200 for 2 glasses of champagne, not too expe...</td>\n",
       "      <td>{'glasses of champagne': -1, 'bottle of wine':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(Always ask the bartender for the SEASONAL bee...</td>\n",
       "      <td>{'SEASONAL beer': 1, 'bartender': 0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(and I have eaten my share) Which impresses me...</td>\n",
       "      <td>{'serve': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(food was delivered by a busboy, not waiter) W...</td>\n",
       "      <td>{'food': 0, 'busboy': -1, 'waiter': -1, 'chees...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>- the bread at the beginning is super tasty an...</td>\n",
       "      <td>{'bread': 1, 'pizza': 1, 'margarite pizza with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20 minutes for our reservation but it gave us ...</td>\n",
       "      <td>{'reservation': -1, 'cocktails': 1, 'surroundi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  $160 for 2 filets, 2 sides, an appetizer and d...   \n",
       "1    $20 for all you can eat sushi cannot be beaten.   \n",
       "2  $20 gets you unlimited sushi of a very high qu...   \n",
       "3  $6 and there is much tasty food, all of it fre...   \n",
       "4  ($200 for 2 glasses of champagne, not too expe...   \n",
       "5  (Always ask the bartender for the SEASONAL bee...   \n",
       "6  (and I have eaten my share) Which impresses me...   \n",
       "7  (food was delivered by a busboy, not waiter) W...   \n",
       "8  - the bread at the beginning is super tasty an...   \n",
       "9  20 minutes for our reservation but it gave us ...   \n",
       "\n",
       "                                                json  \n",
       "0  {'filets': 0, 'sides': 0, 'appetizer': 0, 'dri...  \n",
       "1                                       {'sushi': 0}  \n",
       "2      {'sushi': 1, 'sushi places': 1, 'quality': 1}  \n",
       "3                                        {'food': 1}  \n",
       "4  {'glasses of champagne': -1, 'bottle of wine':...  \n",
       "5               {'SEASONAL beer': 1, 'bartender': 0}  \n",
       "6                                       {'serve': 1}  \n",
       "7  {'food': 0, 'busboy': -1, 'waiter': -1, 'chees...  \n",
       "8  {'bread': 1, 'pizza': 1, 'margarite pizza with...  \n",
       "9  {'reservation': -1, 'cocktails': 1, 'surroundi...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_json = create_json(train)\n",
    "test_json = create_json(test)\n",
    "train_json.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdb1e957-6616-43c7-b098-2c32b3dcf4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dspy = [dspy.Example(review=row['text'], aspects_with_label=row['json']).with_inputs('review') for _, row in train_json.iterrows()]\n",
    "test_dspy = [dspy.Example(review=row['text'], aspects_with_label=row['json']).with_inputs('review') for _, row in test_json.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7f150c5-dc59-44dc-a8e0-e7fb38076530",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "class Review2Aspects(dspy.Signature):\n",
    "    \"\"\"\n",
    "    Identify aspects and their sentiments from a customer review. The aspects must be words or phrases in the review.\n",
    "    The response should be a Python dictionary, where each key is an aspect and the value is a sentiment label.\n",
    "    A label of 1 indicates positive sentiment, 0 indicates neutral sentiment, and -1 indicates negative sentiment.\n",
    "    \"\"\"\n",
    "\n",
    "    review: str = dspy.InputField(desc=\"a customer review\")\n",
    "    aspects_with_label: dict = dspy.OutputField(format=dict, desc=dedent(\"\"\"\n",
    "        a single Python dictionary, where each key is an aspect and the value is the label,\n",
    "        with label 1 indicating positive sentiment, 0 indicating neutral sentiment, and -1\n",
    "        indicating negative sentiment.\n",
    "        \"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afec4bfa-930f-49fd-a8d3-f5e71948ddfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ABSAPipeline(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate_aspects = dspy.TypedChainOfThought(Review2Aspects)\n",
    "\n",
    "    def forward(self, review):\n",
    "        pred = self.generate_aspects(review=review)\n",
    "        return dspy.Prediction(aspects_with_label=pred.aspects_with_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e2e50d8-043b-4986-9ac7-104fcdc02e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_venn(example, pred, trace=None):\n",
    "    try:\n",
    "        total_count = len(merge_jsons([example.aspects_with_label, pred.aspects_with_label]).keys())\n",
    "    except:\n",
    "        print([example.aspects_with_label, pred.aspects_with_label])\n",
    "    accurate_count = sum(1 for k, v in example.aspects_with_label.items() if pred.aspects_with_label.get(k) == v)\n",
    "    if trace is None:\n",
    "        return float(accurate_count) / total_count\n",
    "    else:\n",
    "        return accurate_count > total_count * 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "589b9a88-9e20-46ee-8443-e7e3a27d43fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to sample between 1 and 4 traces per predictor.\n",
      "Will attempt to train 16 candidate sets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 5.483333333333333 / 30  (18.3): 100%|‚ñà| 30/30 [00:20<00:00,  1.45i\n",
      "/Users/richy/.pyenv/versions/3.11.7/envs/llm/lib/python3.11/site-packages/dspy/evaluate/evaluate.py:187: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(truncate_cell)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 5.483333333333333 / 30  (18.3%)\n",
      "Score: 18.28 for set: [0]\n",
      "New best score: 18.28 for seed -3\n",
      "Scores so far: [18.28]\n",
      "Best score: 18.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 21.75 / 30  (72.5): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:07<00:00,  4.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 21.75 / 30  (72.5%)\n",
      "Score: 72.5 for set: [16]\n",
      "New best score: 72.5 for seed -2\n",
      "Scores so far: [18.28, 72.5]\n",
      "Best score: 72.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                     | 16/30 [00:13<00:11,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 17 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 20.644444444444446 / 30  (68.8): 100%|‚ñà| 30/30 [00:08<00:00,  3.61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 20.644444444444446 / 30  (68.8%)\n",
      "Score: 68.81 for set: [16]\n",
      "Scores so far: [18.28, 72.5, 68.81]\n",
      "Best score: 72.5\n",
      "Average of max per entry across top 1 scores: 0.725\n",
      "Average of max per entry across top 2 scores: 0.8555555555555556\n",
      "Average of max per entry across top 3 scores: 0.8555555555555556\n",
      "Average of max per entry across top 5 scores: 0.8555555555555556\n",
      "Average of max per entry across top 8 scores: 0.8555555555555556\n",
      "Average of max per entry across top 9999 scores: 0.8555555555555556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 27/30 [00:37<00:04,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 28 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.0 / 3  (66.7):  10%|‚ñà‚ñé           | 3/30 [00:01<00:15,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.083333333333334 / 8  (51.0):  27%|‚ñé| 8/30 [00:07<00:20,  1.06it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.0 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.1 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.3 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 5.416666666666667 / 10  (54.2):  33%|‚ñé| 10/30 [00:09<00:21,  1.06s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.9 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 3.0 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.8 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 2.1 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.8 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.150000000000002 / 15  (61.0):  50%|‚ñå| 15/30 [00:14<00:13,  1.10i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 2.4 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.9 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.650000000000002 / 16  (60.3):  53%|‚ñå| 16/30 [00:15<00:12,  1.16i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.3 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 10.650000000000002 / 17  (62.6):  57%|‚ñå| 17/30 [00:15<00:10,  1.29"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.650000000000002 / 21  (60.2):  70%|‚ñã| 21/30 [00:19<00:07,  1.15"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 3.0 seconds after 4 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.4 seconds after 4 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.650000000000002 / 22  (62.0):  73%|‚ñã| 22/30 [00:20<00:07,  1.13"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.2 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.7 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.983333333333336 / 25  (59.9):  83%|‚ñä| 25/30 [00:23<00:04,  1.01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 11.2 seconds after 5 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.983333333333336 / 27  (59.2):  90%|‚ñâ| 27/30 [00:27<00:03,  1.28"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.427777777777777 / 30  (61.4): 100%|‚ñà| 30/30 [00:36<00:00,  1.22\n",
      "/Users/richy/.pyenv/versions/3.11.7/envs/llm/lib/python3.11/site-packages/dspy/evaluate/evaluate.py:187: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(truncate_cell)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.427777777777777 / 30  (61.4%)\n",
      "Score: 61.43 for set: [16]\n",
      "Scores so far: [18.28, 72.5, 68.81, 61.43]\n",
      "Best score: 72.5\n",
      "Average of max per entry across top 1 scores: 0.725\n",
      "Average of max per entry across top 2 scores: 0.8555555555555556\n",
      "Average of max per entry across top 3 scores: 0.95\n",
      "Average of max per entry across top 5 scores: 0.95\n",
      "Average of max per entry across top 8 scores: 0.95\n",
      "Average of max per entry across top 9999 scores: 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                   | 7/30 [00:08<00:28,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 8 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.333333333333333 / 3  (77.8):  10%| | 3/30 [00:02<00:19,  1.42it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.8 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.583333333333333 / 4  (64.6):  13%|‚ñè| 4/30 [00:03<00:26,  1.04s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.7 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.5 / 7  (64.3):  23%|‚ñà‚ñà‚ñà          | 7/30 [00:05<00:16,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 1.2 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 1.5 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 6.583333333333333 / 11  (59.8):  37%|‚ñé| 11/30 [00:09<00:13,  1.42i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 2.4 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 1.2 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.583333333333333 / 14  (54.2):  47%|‚ñç| 14/30 [00:13<00:18,  1.13s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.6 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 1.3 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 10.083333333333332 / 18  (56.0):  60%|‚ñå| 18/30 [00:16<00:10,  1.16"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 3.7 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.1 seconds after 4 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.8 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 8.0 seconds after 4 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 11.083333333333332 / 21  (52.8):  70%|‚ñã| 21/30 [00:20<00:09,  1.08"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.1 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.8 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.583333333333332 / 24  (56.6):  80%|‚ñä| 24/30 [00:22<00:05,  1.07"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.5 seconds after 4 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 1.1 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.916666666666664 / 30  (56.4): 100%|‚ñà| 30/30 [00:30<00:00,  1.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.916666666666664 / 30  (56.4%)\n",
      "Score: 56.39 for set: [16]\n",
      "Scores so far: [18.28, 72.5, 68.81, 61.43, 56.39]\n",
      "Best score: 72.5\n",
      "Average of max per entry across top 1 scores: 0.725\n",
      "Average of max per entry across top 2 scores: 0.8555555555555556\n",
      "Average of max per entry across top 3 scores: 0.95\n",
      "Average of max per entry across top 5 scores: 0.9833333333333333\n",
      "Average of max per entry across top 8 scores: 0.9833333333333333\n",
      "Average of max per entry across top 9999 scores: 0.9833333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                       | 4/30 [00:05<00:36,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.0 / 4  (75.0):  13%|‚ñà‚ñã           | 4/30 [00:03<00:22,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.3 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.8 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.5 / 8  (56.2):  27%|‚ñà‚ñà‚ñà‚ñç         | 8/30 [00:07<00:17,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.5 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 1.1 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 1.6 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 1.6 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.866666666666665 / 15  (59.1):  50%|‚ñå| 15/30 [00:12<00:11,  1.30i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 2.7 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.866666666666665 / 16  (55.4):  53%|‚ñå| 16/30 [00:13<00:10,  1.36i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.8 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 10.366666666666665 / 18  (57.6):  60%|‚ñå| 18/30 [00:14<00:07,  1.56"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 3.7 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.366666666666665 / 23  (62.5):  77%|‚ñä| 23/30 [00:17<00:04,  1.64"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 2.1 seconds after 4 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.1 seconds after 4 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 1.6 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.50952380952381 / 27  (61.1):  90%|‚ñâ| 27/30 [00:21<00:02,  1.11i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 5.2 seconds after 4 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.50952380952381 / 30  (55.0): 100%|‚ñà| 30/30 [00:28<00:00,  1.06i\n",
      "/Users/richy/.pyenv/versions/3.11.7/envs/llm/lib/python3.11/site-packages/dspy/evaluate/evaluate.py:187: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(truncate_cell)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.50952380952381 / 30  (55.0%)\n",
      "Score: 55.03 for set: [16]\n",
      "Scores so far: [18.28, 72.5, 68.81, 61.43, 56.39, 55.03]\n",
      "Best score: 72.5\n",
      "Average of max per entry across top 1 scores: 0.725\n",
      "Average of max per entry across top 2 scores: 0.8555555555555556\n",
      "Average of max per entry across top 3 scores: 0.95\n",
      "Average of max per entry across top 5 scores: 1.0\n",
      "Average of max per entry across top 8 scores: 1.0\n",
      "Average of max per entry across top 9999 scores: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñà‚ñà‚ñà‚ñà‚ñå                                         | 3/30 [00:02<00:25,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 4 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.833333333333333 / 4  (70.8):  13%|‚ñè| 4/30 [00:02<00:11,  2.17it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.833333333333333 / 5  (76.7):  17%|‚ñè| 5/30 [00:03<00:20,  1.20it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 1.0 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.8 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.833333333333333 / 6  (80.6):  20%|‚ñè| 6/30 [00:05<00:26,  1.10s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.0 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 1.1 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 1.8 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 5.458333333333333 / 7  (78.0):  23%|‚ñè| 7/30 [00:05<00:22,  1.00it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.8 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 6.458333333333333 / 9  (71.8):  30%|‚ñé| 9/30 [00:07<00:21,  1.02s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.3 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.7 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.458333333333333 / 10  (74.6):  33%|‚ñé| 10/30 [00:09<00:24,  1.21s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {'max_tokens': 2048, 'n': 1, 'temperature': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.458333333333332 / 11  (76.9):  37%|‚ñé| 11/30 [00:11<00:25,  1.34s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.5 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {'max_tokens': 2048, 'n': 1, 'temperature': 0.0}\n",
      "Backing off 0.4 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.058333333333332 / 12  (75.5):  40%|‚ñç| 12/30 [00:13<00:31,  1.76s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.3 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.391666666666666 / 13  (72.2):  43%|‚ñç| 13/30 [00:14<00:22,  1.35s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 6.4 seconds after 4 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 10.725 / 15  (71.5):  50%|‚ñà‚ñà‚ñà‚ñà    | 15/30 [00:16<00:16,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 11.225 / 17  (66.0):  57%|‚ñà‚ñà‚ñà‚ñà‚ñå   | 17/30 [00:18<00:12,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.2 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.5 seconds after 4 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.225 / 19  (64.3):  63%|‚ñà‚ñà‚ñà‚ñà‚ñà   | 19/30 [00:21<00:14,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.5 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 3.0 seconds after 5 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.291666666666666 / 21  (63.3):  70%|‚ñã| 21/30 [00:23<00:10,  1.19"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.3 seconds after 4 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 3.2 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.958333333333332 / 25  (67.8):  83%|‚ñä| 25/30 [00:28<00:04,  1.06"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 15.1 seconds after 5 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.791666666666664 / 30  (66.0): 100%|‚ñà| 30/30 [00:46<00:00,  1.54\n",
      "/Users/richy/.pyenv/versions/3.11.7/envs/llm/lib/python3.11/site-packages/dspy/evaluate/evaluate.py:187: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(truncate_cell)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.791666666666664 / 30  (66.0%)\n",
      "Score: 65.97 for set: [16]\n",
      "Scores so far: [18.28, 72.5, 68.81, 61.43, 56.39, 55.03, 65.97]\n",
      "Best score: 72.5\n",
      "Average of max per entry across top 1 scores: 0.725\n",
      "Average of max per entry across top 2 scores: 0.8555555555555556\n",
      "Average of max per entry across top 3 scores: 0.9388888888888889\n",
      "Average of max per entry across top 5 scores: 0.9888888888888889\n",
      "Average of max per entry across top 8 scores: 1.0\n",
      "Average of max per entry across top 9999 scores: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                 | 8/30 [00:08<00:24,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 9 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 6.749999999999999 / 10  (67.5):  33%|‚ñé| 10/30 [00:02<00:05,  3.40i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.749999999999999 / 11  (70.5):  37%|‚ñé| 11/30 [00:03<00:04,  3.89i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.8 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.75 / 13  (75.0):  40%|‚ñà‚ñà‚ñà‚ñà      | 12/30 [00:03<00:07,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.3 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.0 / 17  (76.5):  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 17/30 [00:06<00:06,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.0 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.0 / 18  (77.8):  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 18/30 [00:07<00:08,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.0 / 20  (70.0):  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 20/30 [00:09<00:07,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.0 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {'max_tokens': 2048, 'n': 1, 'temperature': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.5 / 21  (69.0):  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 21/30 [00:10<00:06,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.9 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 1.7 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 3.3 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.166666666666664 / 23  (70.3):  77%|‚ñä| 23/30 [00:11<00:05,  1.32"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.166666666666664 / 25  (72.7):  83%|‚ñä| 25/30 [00:13<00:03,  1.31"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.8 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 2.2 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.566666666666663 / 27  (72.5):  90%|‚ñâ| 27/30 [00:15<00:03,  1.02"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 21.066666666666663 / 30  (70.2): 100%|‚ñà| 30/30 [00:19<00:00,  1.52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 21.066666666666663 / 30  (70.2%)\n",
      "Score: 70.22 for set: [16]\n",
      "Scores so far: [18.28, 72.5, 68.81, 61.43, 56.39, 55.03, 65.97, 70.22]\n",
      "Best score: 72.5\n",
      "Average of max per entry across top 1 scores: 0.725\n",
      "Average of max per entry across top 2 scores: 0.9111111111111111\n",
      "Average of max per entry across top 3 scores: 0.9277777777777777\n",
      "Average of max per entry across top 5 scores: 0.9722222222222222\n",
      "Average of max per entry across top 8 scores: 1.0\n",
      "Average of max per entry across top 9999 scores: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                    | 6/30 [00:06<00:27,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 7 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.0 / 2  (100.0):   7%|‚ñä           | 2/30 [00:01<00:19,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.0 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 5.000000000000001 / 6  (83.3):  20%|‚ñè| 6/30 [00:05<00:21,  1.11it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.8 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.1 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.6 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 6.333333333333334 / 8  (79.2):  27%|‚ñé| 8/30 [00:07<00:21,  1.01it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.3 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.333333333333334 / 9  (81.5):  30%|‚ñé| 9/30 [00:08<00:19,  1.09it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.1 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.3 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 2.0 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 2.2 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.483333333333334 / 12  (79.0):  40%|‚ñç| 12/30 [00:12<00:17,  1.01i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 8.0 seconds after 4 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 1.5 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 10.816666666666668 / 15  (72.1):  50%|‚ñå| 15/30 [00:15<00:13,  1.10"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.8 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.316666666666668 / 17  (72.5):  57%|‚ñå| 17/30 [00:17<00:12,  1.00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.3 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 1.0 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 3.8 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.316666666666668 / 21  (68.2):  70%|‚ñã| 21/30 [00:21<00:09,  1.01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 2.8 seconds after 4 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.56666666666667 / 24  (69.0):  80%|‚ñä| 24/30 [00:24<00:05,  1.05i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 14.3 seconds after 5 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 17.56666666666667 / 25  (70.3):  83%|‚ñä| 25/30 [00:25<00:05,  1.01s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.7 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.56666666666667 / 26  (71.4):  87%|‚ñä| 26/30 [00:26<00:03,  1.06i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 20.56666666666667 / 30  (68.6): 100%|‚ñà| 30/30 [00:40<00:00,  1.36s\n",
      "/Users/richy/.pyenv/versions/3.11.7/envs/llm/lib/python3.11/site-packages/dspy/evaluate/evaluate.py:187: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(truncate_cell)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 20.56666666666667 / 30  (68.6%)\n",
      "Score: 68.56 for set: [16]\n",
      "Scores so far: [18.28, 72.5, 68.81, 61.43, 56.39, 55.03, 65.97, 70.22, 68.56]\n",
      "Best score: 72.5\n",
      "Average of max per entry across top 1 scores: 0.725\n",
      "Average of max per entry across top 2 scores: 0.9111111111111111\n",
      "Average of max per entry across top 3 scores: 0.9277777777777777\n",
      "Average of max per entry across top 5 scores: 0.9888888888888889\n",
      "Average of max per entry across top 8 scores: 1.0\n",
      "Average of max per entry across top 9999 scores: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                       | 4/30 [00:05<00:38,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.566666666666667 / 6  (59.4):  20%|‚ñè| 6/30 [00:03<00:12,  1.94it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 5.233333333333333 / 8  (65.4):  23%|‚ñè| 7/30 [00:03<00:09,  2.53it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 5.566666666666666 / 9  (61.9):  30%|‚ñé| 9/30 [00:04<00:09,  2.23it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.1 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.566666666666666 / 12  (63.1):  40%|‚ñç| 12/30 [00:05<00:09,  1.88i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.65 / 15  (64.3):  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 15/30 [00:07<00:10,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.4 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 1.0 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 10.872222222222222 / 17  (64.0):  57%|‚ñå| 17/30 [00:09<00:09,  1.33"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.4 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 11.872222222222222 / 18  (66.0):  60%|‚ñå| 18/30 [00:10<00:10,  1.14"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.622222222222222 / 19  (66.4):  63%|‚ñã| 19/30 [00:12<00:11,  1.03"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.2 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 1.4 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.3 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.122222222222222 / 22  (68.7):  73%|‚ñã| 22/30 [00:14<00:06,  1.21"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.2 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 1.3 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.12222222222222 / 24  (67.2):  80%|‚ñä| 24/30 [00:16<00:05,  1.18i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.6 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 17.62222222222222 / 26  (67.8):  87%|‚ñä| 26/30 [00:18<00:03,  1.23i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.1 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.37222222222222 / 28  (69.2):  93%|‚ñâ| 28/30 [00:20<00:02,  1.10s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.3 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 21.12222222222222 / 30  (70.4): 100%|‚ñà| 30/30 [00:23<00:00,  1.25i\n",
      "/Users/richy/.pyenv/versions/3.11.7/envs/llm/lib/python3.11/site-packages/dspy/evaluate/evaluate.py:187: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(truncate_cell)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 21.12222222222222 / 30  (70.4%)\n",
      "Score: 70.41 for set: [16]\n",
      "Scores so far: [18.28, 72.5, 68.81, 61.43, 56.39, 55.03, 65.97, 70.22, 68.56, 70.41]\n",
      "Best score: 72.5\n",
      "Average of max per entry across top 1 scores: 0.725\n",
      "Average of max per entry across top 2 scores: 0.9161111111111111\n",
      "Average of max per entry across top 3 scores: 0.975\n",
      "Average of max per entry across top 5 scores: 0.9916666666666667\n",
      "Average of max per entry across top 8 scores: 1.0\n",
      "Average of max per entry across top 9999 scores: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                   | 7/30 [00:08<00:29,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 8 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.3333333333333333 / 2  (66.7):   7%| | 2/30 [00:02<00:28,  1.03s/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.666666666666666 / 7  (66.7):  23%|‚ñè| 7/30 [00:07<00:22,  1.02it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.3 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.1 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.2 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 5.916666666666666 / 9  (65.7):  30%|‚ñé| 9/30 [00:09<00:19,  1.07it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.2 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.716666666666667 / 12  (64.3):  40%|‚ñç| 12/30 [00:13<00:21,  1.19s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.0 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 3.7 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.916666666666667 / 13  (60.9):  43%|‚ñç| 13/30 [00:14<00:18,  1.11s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.5 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.25 / 15  (61.7):  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 15/30 [00:16<00:14,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.6 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.25 / 17  (54.4):  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 17/30 [00:17<00:10,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.8 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 10.25 / 18  (56.9):  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 18/30 [00:18<00:12,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 6.5 seconds after 4 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.25 / 20  (61.2):  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 20/30 [00:20<00:09,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.3 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.9 seconds after 4 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.75 / 23  (64.1):  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 23/30 [00:23<00:06,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.7 seconds after 4 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 1.0 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.75 / 24  (61.5):  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 24/30 [00:24<00:05,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 7.1 seconds after 4 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.25 / 26  (62.5):  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 26/30 [00:27<00:05,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.9 seconds after 5 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.75 / 27  (62.0):  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 27/30 [00:29<00:04,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 12.3 seconds after 5 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.75 / 30  (62.5): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:44<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.75 / 30  (62.5%)\n",
      "Score: 62.5 for set: [16]\n",
      "Scores so far: [18.28, 72.5, 68.81, 61.43, 56.39, 55.03, 65.97, 70.22, 68.56, 70.41, 62.5]\n",
      "Best score: 72.5\n",
      "Average of max per entry across top 1 scores: 0.725\n",
      "Average of max per entry across top 2 scores: 0.9161111111111111\n",
      "Average of max per entry across top 3 scores: 0.975\n",
      "Average of max per entry across top 5 scores: 0.9916666666666667\n",
      "Average of max per entry across top 8 scores: 1.0\n",
      "Average of max per entry across top 9999 scores: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                            | 11/30 [00:22<00:38,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 12 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 11.952380952380953 / 23  (52.0):  77%|‚ñä| 23/30 [00:04<00:01,  4.32"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.452380952380953 / 30  (61.5): 100%|‚ñà| 30/30 [00:09<00:00,  3.07\n",
      "/Users/richy/.pyenv/versions/3.11.7/envs/llm/lib/python3.11/site-packages/dspy/evaluate/evaluate.py:187: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(truncate_cell)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.452380952380953 / 30  (61.5%)\n",
      "Score: 61.51 for set: [16]\n",
      "Scores so far: [18.28, 72.5, 68.81, 61.43, 56.39, 55.03, 65.97, 70.22, 68.56, 70.41, 62.5, 61.51]\n",
      "Best score: 72.5\n",
      "Average of max per entry across top 1 scores: 0.725\n",
      "Average of max per entry across top 2 scores: 0.9161111111111111\n",
      "Average of max per entry across top 3 scores: 0.975\n",
      "Average of max per entry across top 5 scores: 0.9916666666666667\n",
      "Average of max per entry across top 8 scores: 1.0\n",
      "Average of max per entry across top 9999 scores: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                              | 10/30 [00:11<00:23,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 11 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.777777777777778 / 7  (54.0):  23%|‚ñè| 7/30 [00:04<00:16,  1.38it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.027777777777779 / 8  (50.3):  27%|‚ñé| 8/30 [00:05<00:18,  1.21it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.5 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 1.2 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.8 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 6.361111111111112 / 11  (57.8):  37%|‚ñé| 11/30 [00:09<00:18,  1.01i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.861111111111112 / 13  (60.5):  43%|‚ñç| 13/30 [00:11<00:17,  1.04s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.8 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 2.0 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.0 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.86111111111111 / 15  (65.7):  50%|‚ñå| 15/30 [00:12<00:13,  1.08it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.7 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 11.36111111111111 / 17  (66.8):  57%|‚ñå| 17/30 [00:15<00:15,  1.21s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 4.0 seconds after 4 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 7.3 seconds after 4 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.027777777777777 / 19  (63.3):  63%|‚ñã| 19/30 [00:18<00:12,  1.12"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 2.8 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.86111111111111 / 24  (61.9):  80%|‚ñä| 24/30 [00:22<00:04,  1.31i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.1 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.86111111111111 / 25  (63.4):  83%|‚ñä| 25/30 [00:23<00:03,  1.34i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 1.7 seconds after 4 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 17.427777777777777 / 28  (62.2):  93%|‚ñâ| 28/30 [00:27<00:02,  1.07"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 3.2 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}Backing off 0.7 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.427777777777777 / 30  (64.8): 100%|‚ñà| 30/30 [00:30<00:00,  1.03\n",
      "/Users/richy/.pyenv/versions/3.11.7/envs/llm/lib/python3.11/site-packages/dspy/evaluate/evaluate.py:187: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(truncate_cell)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.427777777777777 / 30  (64.8%)\n",
      "Score: 64.76 for set: [16]\n",
      "Scores so far: [18.28, 72.5, 68.81, 61.43, 56.39, 55.03, 65.97, 70.22, 68.56, 70.41, 62.5, 61.51, 64.76]\n",
      "Best score: 72.5\n",
      "Average of max per entry across top 1 scores: 0.725\n",
      "Average of max per entry across top 2 scores: 0.9161111111111111\n",
      "Average of max per entry across top 3 scores: 0.975\n",
      "Average of max per entry across top 5 scores: 0.9916666666666667\n",
      "Average of max per entry across top 8 scores: 1.0\n",
      "Average of max per entry across top 9999 scores: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                    | 6/30 [00:07<00:31,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 7 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.5555555555555554 / 4  (38.9):  13%|‚ñè| 4/30 [00:03<00:18,  1.40it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.0 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 5.805555555555555 / 9  (64.5):  30%|‚ñé| 9/30 [00:05<00:14,  1.45it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.3 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 6.555555555555555 / 10  (65.6):  33%|‚ñé| 10/30 [00:06<00:15,  1.29i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 1.7 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.555555555555555 / 13  (65.8):  43%|‚ñç| 13/30 [00:09<00:14,  1.15i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.222222222222221 / 14  (65.9):  47%|‚ñç| 14/30 [00:11<00:16,  1.05s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.3 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 3.0 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 10.222222222222221 / 15  (68.1):  50%|‚ñå| 15/30 [00:11<00:14,  1.05"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {'max_tokens': 2048, 'n': 1, 'temperature': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 11.055555555555555 / 17  (65.0):  57%|‚ñå| 17/30 [00:13<00:12,  1.05"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {'max_tokens': 2048, 'n': 1, 'temperature': 0.0}\n",
      "Backing off 1.8 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 2.0 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.38888888888889 / 19  (65.2):  63%|‚ñã| 19/30 [00:16<00:13,  1.21s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.38888888888889 / 20  (61.9):  67%|‚ñã| 20/30 [00:17<00:10,  1.07s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.4 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {'max_tokens': 2048, 'n': 1, 'temperature': 0.0}\n",
      "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {'max_tokens': 2048, 'n': 1, 'temperature': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.13888888888889 / 23  (65.8):  77%|‚ñä| 23/30 [00:20<00:06,  1.09i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.6 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.63888888888889 / 24  (65.2):  80%|‚ñä| 24/30 [00:21<00:05,  1.01i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.3 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.9 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {'max_tokens': 2048, 'n': 1, 'temperature': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 17.88888888888889 / 28  (63.9):  93%|‚ñâ| 28/30 [00:26<00:02,  1.10s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.5 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.88888888888889 / 30  (66.3): 100%|‚ñà| 30/30 [00:29<00:00,  1.03i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.88888888888889 / 30  (66.3%)\n",
      "Score: 66.3 for set: [16]\n",
      "Scores so far: [18.28, 72.5, 68.81, 61.43, 56.39, 55.03, 65.97, 70.22, 68.56, 70.41, 62.5, 61.51, 64.76, 66.3]\n",
      "Best score: 72.5\n",
      "Average of max per entry across top 1 scores: 0.725\n",
      "Average of max per entry across top 2 scores: 0.9161111111111111\n",
      "Average of max per entry across top 3 scores: 0.975\n",
      "Average of max per entry across top 5 scores: 0.9916666666666667\n",
      "Average of max per entry across top 8 scores: 1.0\n",
      "Average of max per entry across top 9999 scores: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                | 19/30 [00:23<00:13,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 20 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.9444444444444444 / 4  (23.6):  13%|‚ñè| 4/30 [00:05<00:36,  1.40s/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.0 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.4444444444444446 / 7  (49.2):  23%|‚ñè| 7/30 [00:08<00:23,  1.02s/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.0 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 6.194444444444445 / 10  (61.9):  33%|‚ñé| 10/30 [00:11<00:22,  1.14s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.8 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 1.5 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.194444444444445 / 12  (68.3):  40%|‚ñç| 12/30 [00:14<00:20,  1.16s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 2.0 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 1.0 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.394444444444444 / 13  (64.6):  43%|‚ñç| 13/30 [00:15<00:19,  1.14s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.9 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 1.5 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.727777777777778 / 14  (62.3):  47%|‚ñç| 14/30 [00:16<00:19,  1.23s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 3.5 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 10.727777777777778 / 16  (67.0):  53%|‚ñå| 16/30 [00:18<00:16,  1.16"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.1 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 11.727777777777778 / 17  (69.0):  57%|‚ñå| 17/30 [00:20<00:16,  1.26"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.0 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.727777777777778 / 18  (70.7):  60%|‚ñå| 18/30 [00:21<00:15,  1.27"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.7 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.227777777777778 / 22  (69.2):  73%|‚ñã| 22/30 [00:25<00:07,  1.04"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 2.1 seconds after 4 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.227777777777778 / 24  (67.6):  80%|‚ñä| 24/30 [00:27<00:05,  1.10"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.7 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 17.227777777777778 / 25  (68.9):  83%|‚ñä| 25/30 [00:28<00:04,  1.06"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.4 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 1.2 seconds after 4 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.477777777777778 / 28  (66.0):  93%|‚ñâ| 28/30 [00:31<00:02,  1.11"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 3.1 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.81111111111111 / 30  (62.7): 100%|‚ñà| 30/30 [00:36<00:00,  1.22s\n",
      "/Users/richy/.pyenv/versions/3.11.7/envs/llm/lib/python3.11/site-packages/dspy/evaluate/evaluate.py:187: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(truncate_cell)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.81111111111111 / 30  (62.7%)\n",
      "Score: 62.7 for set: [16]\n",
      "Scores so far: [18.28, 72.5, 68.81, 61.43, 56.39, 55.03, 65.97, 70.22, 68.56, 70.41, 62.5, 61.51, 64.76, 66.3, 62.7]\n",
      "Best score: 72.5\n",
      "Average of max per entry across top 1 scores: 0.725\n",
      "Average of max per entry across top 2 scores: 0.9161111111111111\n",
      "Average of max per entry across top 3 scores: 0.975\n",
      "Average of max per entry across top 5 scores: 0.9916666666666667\n",
      "Average of max per entry across top 8 scores: 1.0\n",
      "Average of max per entry across top 9999 scores: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                        | 14/30 [00:17<00:19,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 15 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.0 / 3  (100.0):  10%|‚ñà‚ñè          | 3/30 [00:03<00:30,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 5.0 / 6  (83.3):  20%|‚ñà‚ñà‚ñå          | 6/30 [00:06<00:26,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.7 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 5.333333333333333 / 7  (76.2):  23%|‚ñè| 7/30 [00:07<00:23,  1.02s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.2 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}Backing off 1.5 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "\n",
      "Backing off 1.4 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.5 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.916666666666666 / 12  (74.3):  40%|‚ñç| 12/30 [00:12<00:16,  1.08i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.4 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}Backing off 1.4 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "\n",
      "Backing off 1.7 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 10.75 / 16  (67.2):  53%|‚ñà‚ñà‚ñà‚ñà‚ñä    | 16/30 [00:15<00:12,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.2 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 2.5 seconds after 4 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 7.0 seconds after 4 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.916666666666666 / 21  (61.5):  70%|‚ñã| 21/30 [00:20<00:09,  1.03"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.0 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.916666666666666 / 22  (63.3):  73%|‚ñã| 22/30 [00:21<00:07,  1.05"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.6 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.916666666666666 / 23  (60.5):  77%|‚ñä| 23/30 [00:22<00:06,  1.10"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 15.9 seconds after 5 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.416666666666666 / 24  (60.1):  80%|‚ñä| 24/30 [00:23<00:05,  1.04"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.166666666666664 / 26  (62.2):  87%|‚ñä| 26/30 [00:28<00:06,  1.68"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.1 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.166666666666664 / 30  (60.6): 100%|‚ñà| 30/30 [00:40<00:00,  1.35\n",
      "/Users/richy/.pyenv/versions/3.11.7/envs/llm/lib/python3.11/site-packages/dspy/evaluate/evaluate.py:187: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(truncate_cell)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 18.166666666666664 / 30  (60.6%)\n",
      "Score: 60.56 for set: [16]\n",
      "Scores so far: [18.28, 72.5, 68.81, 61.43, 56.39, 55.03, 65.97, 70.22, 68.56, 70.41, 62.5, 61.51, 64.76, 66.3, 62.7, 60.56]\n",
      "Best score: 72.5\n",
      "Average of max per entry across top 1 scores: 0.725\n",
      "Average of max per entry across top 2 scores: 0.9161111111111111\n",
      "Average of max per entry across top 3 scores: 0.975\n",
      "Average of max per entry across top 5 scores: 0.9916666666666667\n",
      "Average of max per entry across top 8 scores: 1.0\n",
      "Average of max per entry across top 9999 scores: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            | 22/30 [00:29<00:10,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 23 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.466666666666667 / 9  (49.6):  30%|‚ñé| 9/30 [00:04<00:09,  2.18it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.8 / 10  (48.0):  33%|‚ñà‚ñà‚ñà‚ñã       | 10/30 [00:04<00:08,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.3 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.883333333333335 / 15  (59.2):  50%|‚ñå| 15/30 [00:07<00:08,  1.80i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.1 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.8 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.883333333333335 / 16  (55.5):  53%|‚ñå| 16/30 [00:08<00:08,  1.71i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.5 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.883333333333335 / 18  (49.4):  60%|‚ñå| 18/30 [00:09<00:07,  1.55i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 3.7 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.9 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.5 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 11.383333333333335 / 22  (51.7):  73%|‚ñã| 22/30 [00:13<00:06,  1.27"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 3.0 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.8 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.8 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 1.9 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.383333333333335 / 24  (55.8):  80%|‚ñä| 24/30 [00:15<00:04,  1.21"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.9 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.5 seconds after 4 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.383333333333335 / 25  (57.5):  83%|‚ñä| 25/30 [00:16<00:04,  1.09"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.5 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 2.2 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 14.883333333333335 / 26  (57.2):  87%|‚ñä| 26/30 [00:18<00:04,  1.21"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 2.5 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 10.1 seconds after 5 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.55 / 30  (55.2): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:29<00:00,  1.00it/s]\n",
      "/Users/richy/.pyenv/versions/3.11.7/envs/llm/lib/python3.11/site-packages/dspy/evaluate/evaluate.py:187: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(truncate_cell)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.55 / 30  (55.2%)\n",
      "Score: 55.17 for set: [16]\n",
      "Scores so far: [18.28, 72.5, 68.81, 61.43, 56.39, 55.03, 65.97, 70.22, 68.56, 70.41, 62.5, 61.51, 64.76, 66.3, 62.7, 60.56, 55.17]\n",
      "Best score: 72.5\n",
      "Average of max per entry across top 1 scores: 0.725\n",
      "Average of max per entry across top 2 scores: 0.9161111111111111\n",
      "Average of max per entry across top 3 scores: 0.975\n",
      "Average of max per entry across top 5 scores: 0.9916666666666667\n",
      "Average of max per entry across top 8 scores: 1.0\n",
      "Average of max per entry across top 9999 scores: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñà‚ñå                                            | 1/30 [00:00<00:25,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.344444444444445 / 9  (48.3):  30%|‚ñé| 9/30 [00:03<00:07,  2.82it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 5.411111111111112 / 11  (49.2):  33%|‚ñé| 10/30 [00:05<00:11,  1.71i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.411111111111112 / 14  (52.9):  47%|‚ñç| 14/30 [00:08<00:11,  1.42i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.9 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 7.744444444444445 / 15  (51.6):  50%|‚ñå| 15/30 [00:09<00:13,  1.13i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 1.8 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 10.744444444444445 / 18  (59.7):  60%|‚ñå| 18/30 [00:11<00:10,  1.16"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}Backing off 1.6 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "\n",
      "Backing off 0.8 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {'max_tokens': 2048, 'n': 1, 'temperature': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 10.744444444444445 / 19  (56.5):  63%|‚ñã| 19/30 [00:13<00:10,  1.05"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.1 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 12.244444444444445 / 21  (58.3):  70%|‚ñã| 21/30 [00:14<00:08,  1.08"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.2 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 13.244444444444445 / 22  (60.2):  73%|‚ñã| 22/30 [00:15<00:06,  1.23"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {'max_tokens': 2048, 'n': 1, 'temperature': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.244444444444445 / 24  (63.5):  80%|‚ñä| 24/30 [00:17<00:05,  1.02"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.4 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.6 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 3.6 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 17.744444444444447 / 27  (65.7):  90%|‚ñâ| 27/30 [00:20<00:02,  1.10"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.8 seconds after 4 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.744444444444447 / 30  (65.8): 100%|‚ñà| 30/30 [00:25<00:00,  1.18\n",
      "/Users/richy/.pyenv/versions/3.11.7/envs/llm/lib/python3.11/site-packages/dspy/evaluate/evaluate.py:187: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(truncate_cell)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 19.744444444444447 / 30  (65.8%)\n",
      "Score: 65.81 for set: [16]\n",
      "Scores so far: [18.28, 72.5, 68.81, 61.43, 56.39, 55.03, 65.97, 70.22, 68.56, 70.41, 62.5, 61.51, 64.76, 66.3, 62.7, 60.56, 55.17, 65.81]\n",
      "Best score: 72.5\n",
      "Average of max per entry across top 1 scores: 0.725\n",
      "Average of max per entry across top 2 scores: 0.9161111111111111\n",
      "Average of max per entry across top 3 scores: 0.975\n",
      "Average of max per entry across top 5 scores: 0.9916666666666667\n",
      "Average of max per entry across top 8 scores: 1.0\n",
      "Average of max per entry across top 9999 scores: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                      | 5/30 [00:05<00:28,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 6 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.1944444444444446 / 3  (73.1):  10%| | 3/30 [00:03<00:35,  1.31s/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 1.0 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.7 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 1.0 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3.4444444444444446 / 5  (68.9):  17%|‚ñè| 5/30 [00:06<00:30,  1.21s/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.0 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 4.444444444444445 / 6  (74.1):  20%|‚ñè| 6/30 [00:06<00:25,  1.07s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.0 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 1.9 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 1.2 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 1.6 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.527777777777779 / 12  (71.1):  40%|‚ñç| 12/30 [00:11<00:15,  1.18i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 3.6 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 1.9 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.8 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.0 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 9.861111111111112 / 14  (70.4):  47%|‚ñç| 14/30 [00:13<00:14,  1.14i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.3 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 10.361111111111112 / 15  (69.1):  50%|‚ñå| 15/30 [00:14<00:12,  1.17"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.7 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.5 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 10.861111111111112 / 16  (67.9):  53%|‚ñå| 16/30 [00:15<00:12,  1.10"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 1.1 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.1 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 11.694444444444446 / 18  (65.0):  60%|‚ñå| 18/30 [00:17<00:12,  1.06"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.6 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 1.0 seconds after 2 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.5 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 1.3 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 3.6 seconds after 3 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 16.444444444444446 / 23  (71.5):  77%|‚ñä| 23/30 [00:21<00:06,  1.05"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing off 0.2 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {'max_tokens': 2048, 'n': 1, 'temperature': 0.0}\n",
      "Backing off 0.6 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 0.3 seconds after 1 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n",
      "Backing off 5.4 seconds after 4 tries calling function <function GPT3.request at 0x10dedca40> with kwargs {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 21.944444444444446 / 30  (73.1): 100%|‚ñà| 30/30 [00:29<00:00,  1.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 21.944444444444446 / 30  (73.1%)\n",
      "Score: 73.15 for set: [16]\n",
      "New best score: 73.15 for seed 15\n",
      "Scores so far: [18.28, 72.5, 68.81, 61.43, 56.39, 55.03, 65.97, 70.22, 68.56, 70.41, 62.5, 61.51, 64.76, 66.3, 62.7, 60.56, 55.17, 65.81, 73.15]\n",
      "Best score: 73.15\n",
      "Average of max per entry across top 1 scores: 0.7314814814814815\n",
      "Average of max per entry across top 2 scores: 0.9077777777777778\n",
      "Average of max per entry across top 3 scores: 0.9466666666666667\n",
      "Average of max per entry across top 5 scores: 0.975\n",
      "Average of max per entry across top 8 scores: 1.0\n",
      "Average of max per entry across top 9999 scores: 1.0\n",
      "19 candidate programs found.\n",
      "{'menu': 1, 'pub fare': 1, 'burgers': 1, 'steaks': 1, 'shepherds pie': 1, 'portabella lasagna': 1, 'vegetarians': 1}\n"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n",
    "\n",
    "gpt = dspy.OpenAI(model=\"gpt-3.5-turbo\", max_tokens=4096)\n",
    "with dspy.settings.context(lm=gpt):\n",
    "    uncompiled_pipeline = ABSAPipeline()\n",
    "    pred = uncompiled_pipeline(test_dspy[0].review)\n",
    "    optimizer = BootstrapFewShotWithRandomSearch(metric=validate_venn)\n",
    "    compiled_pipeline = optimizer.compile(ABSAPipeline(), teacher=ABSAPipeline(), trainset=train_dspy[:30])\n",
    "    pred = compiled_pipeline(test_dspy[0].review)\n",
    "print(pred.aspects_with_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bee8e348-318c-4c8c-8032-da4b6d5ef207",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'strip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dspy\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mcontext(backend\u001b[38;5;241m=\u001b[39mbackend):\n\u001b[1;32m      5\u001b[0m     uncompiled_pipeline \u001b[38;5;241m=\u001b[39m ABSAPipeline()\n\u001b[0;32m----> 6\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43muncompiled_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dspy\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreview\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# optimizer = BootstrapFewShotWithRandomSearch(metric=validate_venn)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# compiled_pipeline = optimizer.compile(ABSAPipeline(), teacher=ABSAPipeline(), trainset=train_dspy[:30])\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# pred = compiled_pipeline(test_dspy[0].review)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(pred\u001b[38;5;241m.\u001b[39maspects_with_label)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/dspy-backend/lib/python3.11/site-packages/openinference/instrumentation/dspy/__init__.py:373\u001b[0m, in \u001b[0;36m_ModuleForwardWrapper.__call__\u001b[0;34m(self, wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tracer\u001b[38;5;241m.\u001b[39mstart_as_current_span(\n\u001b[1;32m    354\u001b[0m     span_name,\n\u001b[1;32m    355\u001b[0m     attributes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    370\u001b[0m     ),\n\u001b[1;32m    371\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m span:\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 373\u001b[0m         prediction \u001b[38;5;241m=\u001b[39m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m    375\u001b[0m         span\u001b[38;5;241m.\u001b[39mset_status(trace_api\u001b[38;5;241m.\u001b[39mStatus(trace_api\u001b[38;5;241m.\u001b[39mStatusCode\u001b[38;5;241m.\u001b[39mERROR, \u001b[38;5;28mstr\u001b[39m(exception)))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/dspy-backend/lib/python3.11/site-packages/dspy/primitives/program.py:26\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m, in \u001b[0;36mABSAPipeline.forward\u001b[0;34m(self, review)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, review):\n\u001b[0;32m----> 7\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_aspects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreview\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreview\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dspy\u001b[38;5;241m.\u001b[39mPrediction(aspects_with_label\u001b[38;5;241m=\u001b[39mpred\u001b[38;5;241m.\u001b[39maspects_with_label)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/dspy-backend/lib/python3.11/site-packages/openinference/instrumentation/dspy/__init__.py:373\u001b[0m, in \u001b[0;36m_ModuleForwardWrapper.__call__\u001b[0;34m(self, wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tracer\u001b[38;5;241m.\u001b[39mstart_as_current_span(\n\u001b[1;32m    354\u001b[0m     span_name,\n\u001b[1;32m    355\u001b[0m     attributes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    370\u001b[0m     ),\n\u001b[1;32m    371\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m span:\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 373\u001b[0m         prediction \u001b[38;5;241m=\u001b[39m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m    375\u001b[0m         span\u001b[38;5;241m.\u001b[39mset_status(trace_api\u001b[38;5;241m.\u001b[39mStatus(trace_api\u001b[38;5;241m.\u001b[39mStatusCode\u001b[38;5;241m.\u001b[39mERROR, \u001b[38;5;28mstr\u001b[39m(exception)))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/dspy-backend/lib/python3.11/site-packages/dspy/primitives/program.py:26\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/dspy-backend/lib/python3.11/site-packages/dspy/functional/functional.py:305\u001b[0m, in \u001b[0;36mTypedPredictor.forward\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m     value \u001b[38;5;241m=\u001b[39m completion[name]\n\u001b[1;32m    304\u001b[0m     parser \u001b[38;5;241m=\u001b[39m field\u001b[38;5;241m.\u001b[39mjson_schema_extra\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m x: x)\n\u001b[0;32m--> 305\u001b[0m     parsed[name] \u001b[38;5;241m=\u001b[39m \u001b[43mparser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (pydantic\u001b[38;5;241m.\u001b[39mValidationError, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    307\u001b[0m     errors[name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_error(\n\u001b[1;32m    308\u001b[0m         e,\n\u001b[1;32m    309\u001b[0m         signature\u001b[38;5;241m.\u001b[39mfields[name],\n\u001b[1;32m    310\u001b[0m         value,\n\u001b[1;32m    311\u001b[0m         lm_explain\u001b[38;5;241m=\u001b[39mtry_i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[1;32m    312\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/dspy-backend/lib/python3.11/site-packages/dspy/functional/functional.py:262\u001b[0m, in \u001b[0;36mTypedPredictor._prepare_signature.<locals>.<lambda>\u001b[0;34m(x, from_json)\u001b[0m\n\u001b[1;32m    255\u001b[0m             to_json \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x, inner\u001b[38;5;241m=\u001b[39mto_json: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```json\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m inner(x) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    256\u001b[0m             schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```json\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m schema \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m```\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    257\u001b[0m         signature \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mwith_updated_fields(\n\u001b[1;32m    258\u001b[0m             name,\n\u001b[1;32m    259\u001b[0m             desc\u001b[38;5;241m=\u001b[39mfield\u001b[38;5;241m.\u001b[39mjson_schema_extra\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdesc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    260\u001b[0m             \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. Respond with a single JSON object. JSON Schema: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m schema),\n\u001b[1;32m    261\u001b[0m             \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x, to_json\u001b[38;5;241m=\u001b[39mto_json: (x \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m to_json(x)),\n\u001b[0;32m--> 262\u001b[0m             parser\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x, from_json\u001b[38;5;241m=\u001b[39mfrom_json: from_json(\u001b[43m_unwrap_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m),\n\u001b[1;32m    263\u001b[0m             type_\u001b[38;5;241m=\u001b[39mtype_,\n\u001b[1;32m    264\u001b[0m         )\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# If input field\u001b[39;00m\n\u001b[1;32m    266\u001b[0m     is_json \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/dspy-backend/lib/python3.11/site-packages/dspy/functional/functional.py:426\u001b[0m, in \u001b[0;36m_unwrap_json\u001b[0;34m(output)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_unwrap_json\u001b[39m(output):\n\u001b[0;32m--> 426\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m()\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```json\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'strip'"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n",
    "\n",
    "backend = dspy.JSONBackend(model=\"gpt-3.5-turbo\")\n",
    "with dspy.settings.context(backend=backend):\n",
    "    uncompiled_pipeline = ABSAPipeline()\n",
    "    pred = uncompiled_pipeline(test_dspy[0].review)\n",
    "    # optimizer = BootstrapFewShotWithRandomSearch(metric=validate_venn)\n",
    "    # compiled_pipeline = optimizer.compile(ABSAPipeline(), teacher=ABSAPipeline(), trainset=train_dspy[:30])\n",
    "    # pred = compiled_pipeline(test_dspy[0].review)\n",
    "print(pred.aspects_with_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "46d05a73-d043-42bb-9bc2-e60a91026e11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===INPUT KWARGS===\n",
      "{ 'messages': [ { 'content': '\\n'\n",
      "                             '    Definition: The output will be the aspects '\n",
      "                             '(both implicit and explicit) and the aspects '\n",
      "                             'sentiment polarity.\\n'\n",
      "                             '    \\n'\n",
      "                             '\\n'\n",
      "                             '---\\n'\n",
      "                             '\\n'\n",
      "                             'Provided the following:\\n'\n",
      "                             'Review: a customer review\\n'\n",
      "                             '\\n'\n",
      "                             'Please return the following fields:\\n'\n",
      "                             'Aspects With Label: \\n'\n",
      "                             '        a single Python dictionary, where each '\n",
      "                             'key is an aspect and the value is the label,\\n'\n",
      "                             '        with label 1 indicating positive '\n",
      "                             'sentiment, 0 indicating neutral sentiment, and '\n",
      "                             '-1\\n'\n",
      "                             '        indicating negative sentiment.\\n'\n",
      "                             '        \\n'\n",
      "                             '\\n'\n",
      "                             'According to the following JSON schema:\\n'\n",
      "                             '{\"properties\": {\"review\": {\"title\": \"Review\", '\n",
      "                             '\"type\": \"string\"}, \"aspects_with_label\": '\n",
      "                             '{\"title\": \"Aspects_with_label\", \"type\": '\n",
      "                             '\"string\"}}, \"required\": [\"review\", '\n",
      "                             '\"aspects_with_label\"]}\\n'\n",
      "                             '\\n'\n",
      "                             '---\\n'\n",
      "                             '\\n'\n",
      "                             '{\"review\": \"\\\\\" The menu includes pub '\n",
      "                             'fare--burgers, steaks and shepherds pie--and '\n",
      "                             'even a portabella lasagna for those black sheep '\n",
      "                             'known as \\\\\"vegetarians.\"}',\n",
      "                  'role': 'user'}]}\n",
      "\n",
      "===COMPLETIONS===\n",
      "Completions(\n",
      "    demos=[[]],\n",
      "    review=['\" The menu includes pub fare--burgers, steaks and shepherds pie--and even a portabella lasagna for those black sheep known as \"vegetarians.'],\n",
      "    aspects_with_label=[{'menu': 0, 'pub fare': 0, 'burgers': 1, 'steaks': 1, 'shepherds pie': 1, 'portabella lasagna': 1, 'vegetarians': 1}]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "backend.inspect_history(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007e8e05-6942-46c8-9b6e-f39ad9f4ec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBackend(dspy.TextBackend):\n",
    "    model: str\n",
    "\n",
    "    def _example_span(self, signature: Signature, example: Example) -> str:\n",
    "        span = {}\n",
    "        for name, _ in signature.fields.items():\n",
    "            if name in example:\n",
    "                span[name] = f\"{example[name]}\"\n",
    "\n",
    "        return json.dumps(span)\n",
    "\n",
    "        \n",
    "\n",
    "    def prepare_request(self, signature: dspy.Signature, example: dspy.Example, config: dict, **_kwargs) -> dict:\n",
    "        options = {**self.STANDARD_PARAMS, **config}\n",
    "\n",
    "        prompt_spans = []\n",
    "\n",
    "        # Start by getting the instructions\n",
    "        prompt_spans.append(signature.instructions)\n",
    "\n",
    "        # Generate Spans for All the demos\n",
    "        for demo in example.demos:\n",
    "            prompt_spans.append(self._example_span(signature, demo))\n",
    "\n",
    "        # Generate Span for the active example\n",
    "        prompt_spans.append(self._example_span(signature, example))\n",
    "\n",
    "        content = \"\\n\\n---\\n\\n\".join([span.strip() for span in prompt_spans])\n",
    "\n",
    "        messages = {\"messages\": [{\"role\": \"user\", \"content\": content}]}\n",
    "\n",
    "        options.update(**messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aff5df72-2c17-4de0-9bb7-ab122f2c7088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('review', FieldInfo(annotation=str, required=True, json_schema_extra={'desc': 'a customer review', '__dspy_field_type': 'input', 'prefix': 'Review:'})), ('aspects_with_label', FieldInfo(annotation=str, required=True, json_schema_extra={'desc': '\\n        a single Python dictionary, where each key is an aspect and the value is the label,\\n        with label 1 indicating positive sentiment, 0 indicating neutral sentiment, and -1\\n        indicating negative sentiment.\\n        ', '__dspy_field_type': 'output', 'prefix': 'Aspects With Label:'}))])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Review2Aspects.fields.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e47a647-5fc7-4890-954b-e8c34433bd3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
